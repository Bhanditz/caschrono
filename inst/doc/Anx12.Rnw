%\VignetteIndexEntry{Annexe Chapitre 12}
%\VignetteDepends{}
%\VignetteKeywords{ts}
%\VignettePackage{caschrono}
\documentclass{article}
\usepackage{Sweave}
\usepackage{times}
\usepackage{mathptm}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{pratiquer}
\setkeys{Gin}{width=0.95\textwidth}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\BB}{BB}
\DeclareMathOperator{\BBN}{BBN}
\DeclareMathOperator{\rang}{rang}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\V}{V}
\DeclareMathOperator{\C}{Cov}
\DeclareMathOperator{\Prob}{Pr}
\DeclareMathOperator{\diag}{diag}
\def\agra{\boldsymbol{a}} % a grave
%
\def\alg{\boldsymbol{\alpha}}
\def\b{\mbox{\bf b}}
%
\def\B{\mbox{\bf B}}
\def\BB#1{\mbox{BB}(0,\sigma_{#1}^2)}
\def\bm{\mbox{B}}
\def\betg{\boldsymbol{\beta}}
\def\bar#1#2{\overline{#1}_{#2}} % surligné indicé
\def\bgr{\mbox{\bf b}}
\def\bla{\mbox{~}}
\def\blb{\mbox{~~}}
%
\def\card{\mbox{card}}
\def\corr{\mbox{corr}}
\def\cov{\mbox{\sf cov}}
\def\cv{\mbox{CV}}
%
\def\D{\mbox{\bf D}}
\def\d{\bf d}
\def\degr{^{\circ}}
\def\deltag{\boldsymbol{\delta}}
\def\Deltag{\boldsymbol{\Delta}} % ligne 64
\def\diag{\mbox{diag}}
\def\dir{\mbox{\tiny{DIR}}}
\def\dsp{\displaystyle}
%
\def\e{\mbox{\bf e}}
\def\eps{{\bf \epsilon}}
\def\esp{\mbox{\sf E}}
\def\eti{\tilde{\epsilon}}

\def\F{\mbox{\bf F}}
\def\g{\mbox{\bf g}}
\def\gamg{\boldsymbol{\Gamma}}
\def\gamgp{\boldsymbol{\gamma}}
\def\hors{\mathbin{\in\mkern-12mu/}}

\def\I{\mbox{\bf I}}
\def\indic{\mathrm{I\mkern-8muI}}
\def\ie{c'est-à-dire}
\def\iid{\sim_{\mbox{i.i.d.}}}
\def\lm{\mbox{L}}

\def\mapsous#1{\smash{ \mathop{\longrightarrow}\limits_{#1}}} %
\def\mapsur#1{\smash{ \mathop{\longrightarrow}\limits^{#1}}}  %
\def\M{\mbox{\bf M}}
\def\mug{\boldsymbol{\mu}}
\def\nor{\mathcal{N}}
\def\nug{\boldsymbol{\nu}}
\def\Nx{\mbox{\bf N}}
\def\P{\mbox{\bf P}}
\def\Phig{\boldsymbol{\Phi}}
\def\phig{\boldsymbol{\phi}}
\def\poi{\mbox{\cal P}}
\def\pr{\mbox{Pr}}
\def\prim{^{\boldsymbol{\prime}}}
\def\px{\mbox{\bf x}}

\def\Rho{\boldsymbol{\rho}}
\def\rl{\mathbin{I\mkern-8muR}} % Reel
\def\RR{\textsf{R}\/}

\newcommand{\sig}[2]{\Sigma_{{#1},{#2}}}
\def \sitst{\textsf{SiteST}\/}
\def\SP{\texttt{S-PLUS}\/}
\def\T{\mbox{\bf T}}
\def\tr{\triangle}
\def\t{\mbox{\bf t}}
\def\tra{\mbox{tr}}

\def\U{\mbox{\bf U}}
\def\Unif{\emph{Unif}}
\def\u{\mbox{\bf u}}
\def\v{\mbox{\bf v}}

\def\w{\mbox{\bf w}}
\def\var{\mbox{\sf var}}
\def\W{\mbox{\bf W}}
\def\X{\textbf{X}}
\def\x{\textbf{x}}

\def\Y{\textbf{Y}}
\def\yg{\textbf{y}}
\def\y0{y^0}

\def\Z{\textbf{Z}}
\def\z{ \textbf{z}}
\def\zer{\large{0}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\urlstyle{sf}
\def\rmdefault{cmr}

%%%% pour les chiffres des équations
\makeatletter
\renewcommand\theequation{\thesection.\arabic{equation}}
\@addtoreset{equation}{section}
\makeatother

\title{Hétéroscédasticité conditionnelle (compléments du Chapitre 12)}
\author{Yves Aragon\footnote{aragon@cict.fr} \cr
{\normalsize Université Toulouse 1 Capitole} }
\begin{document}
\maketitle
\setcounter{section}{12}
\renewcommand{\thefootnote}{\arabic{footnote}}

\SweaveOpts{keep.source=TRUE}
<<echo=FALSE>>=
owidth <- getOption("width") # largeur des sorties
options(width=60, continue="+ ","warn"=-1 )
.PngNo <- 0
nom.fich = "./Figures/anx12-bitmap-"
@
%
@
%
\SweaveOpts{keep.source=TRUE}
%
% les différents types de graphiques
%  ancien
<<label=bfig,echo=FALSE,eval=FALSE>>=
.PngNo <- .PngNo + 1; file = paste(nom.fich, .PngNo, sep="")
pdf(file=paste(file,".pdf",sep=""), width = 7, height = 7, pointsize = 12, bg = "white")
@

<<label=bfigps,echo=FALSE,eval=FALSE>>=
postscript(file=paste(file,".ps",sep=""), width = 7, height = 7, pointsize = 12, bg = "white",horizontal= FALSE,paper="special")
@


% 1111111111111
<<label=bfig1,echo=FALSE,eval=FALSE>>=
.PngNo <- .PngNo + 1; file = paste(nom.fich, .PngNo, sep="")
pdf(file=paste(file,".pdf",sep=""), width = 5, height = 2, pointsize = 10, bg = "white")
@

<<label=bfigps1,echo=FALSE,eval=FALSE>>=
postscript(file=paste(file,".ps",sep=""),  width = 5, height =2, pointsize = 10, bg = "white",horizontal= FALSE,paper="special")
@
% 222222222222222
<<label=bfig2,echo=FALSE,eval=FALSE>>=
.PngNo <- .PngNo + 1; file = paste(nom.fich, .PngNo, sep="")
pdf(file=paste(file,".pdf",sep=""), width = 3.9, height = 3.1, pointsize = 10, bg = "white")
@

<<label=bfigps2,echo=FALSE,eval=FALSE>>=
postscript(file=paste(file,".ps",sep=""), width = 3.9, height = 3.1,   pointsize = 10, bg = "white",horizontal= FALSE,paper="special")
@
%   3333333333333333333333333333

<<label=bfig3,echo=FALSE,eval=FALSE>>=
.PngNo <- .PngNo + 1; file = paste(nom.fich, .PngNo, sep="")
pdf(file=paste(file,".pdf",sep=""), width = 5.92, height = 6.74, pointsize = 10, bg = "white")
@
<<label=bfigps3,echo=FALSE,eval=FALSE>>=
postscript(file=paste(file,".ps",sep=""), width = 5.92, height = 6.74, pointsize = 10, bg = "white",horizontal= FALSE,paper="special")
@

<<label=bfig4,echo=FALSE,eval=FALSE>>=
.PngNo <- .PngNo + 1; file = paste(nom.fich, .PngNo, sep="")
pdf(file=paste(file,".pdf",sep=""), width = 6, height = 6, pointsize = 10, bg = "white")
@
<<label=bfigps4,echo=FALSE,eval=FALSE>>=
postscript(file=paste(file,".ps",sep=""), width = 6, height = 6, pointsize = 10, bg = "white",horizontal= FALSE,paper="special")
@

%5555555555555555
<<label=bfig5,echo=FALSE,eval=FALSE>>=
.PngNo <- .PngNo + 1; file = paste(nom.fich, .PngNo, sep="")
pdf(file=paste(file,".pdf",sep=""), width = 7, height = 4, pointsize = 10, bg = "white")
@

<<label=bfigps5,echo=FALSE,eval=FALSE>>=
postscript(file=paste(file,".ps",sep=""),  width = 7, height =4, pointsize = 10, bg = "white",horizontal= FALSE,paper="special")
@


<<label=zfig2,echo=FALSE,eval=FALSE>>=
dev.null <- dev.off()
@

<<label=zfiginclude,echo=FALSE,eval=FALSE>>=
cat("\\includegraphics[width=0.9\\textwidth]{", file, "}\n\n", sep="")
@


\begin{Exercice}
Faire apparaître A partir du  rendement  composé quotidien $r_t = \log(x_t) - \log(x_{t-1})$ calculer le rendement composé sur une semaine
de 5 jours.
\end{Exercice}

\textbf{Réponse.}
Le rendement composé sur 5 jours est
\begin{multline}
 r_{5,t} = \log(x_t) - \log(x_{t-5}) \\
 = (\log(x_t) - \log(x_{t-1})) + ( \log(x_{t-1})-   \log(x_{t-2})+\cdots +(\log(x_{t-4})-   \log(x_{t-5}))
 \end{multline}
il est donc la somme des rendements quotidiens des jours de la période.



\begin{Exercice}
\begin{itemize}
  \item Simuler 300 observations de $y_t $ obéissant  à  :
  \begin{eqnarray}\label{sim.arch1}
    y_t = 5 +  \epsilon_t   \\
z^2   \epsilon_t = \sigma_t z_t,\; z_t \thicksim \BBN(0,\sigma_z^2) \\
    \sigma^2_t = 0.1 + 0.9 \: \epsilon_{t-1}^2
  \end{eqnarray}
(On simulera 10 valeurs
avant de commencer à stocker la série et on initialisera  à 0 la première
variance conditionnelle.) On reproduira 
explicitement les équations. Visualiser la série et recommencer les simulations avec différentes graines. 
  \item Tester la blancheur du carré centré de la série. Commenter.
  
\end{itemize}

\end{Exercice} 

\textbf{Réponse. } 
Il nous faut essentiellement : (1) tirer les valeurs de $z_t$ i.i.d. $\nor(0,1)$, (2) calculer les variances conditionnelles par récurrence. Après quoi on peut   calculer $y_t$.

{\small
<<label=sim.manu>>=
require(caschrono)
set.seed(1618) ; nobs =300
zt = rnorm(nobs+10)
# initialisation
sig2 = rep(0,nobs+10)
epsil = sig2
y = sig2
for ( i in 2:(nobs+10))
{ sig2[i] = 0.1 + 0.9*epsil[i-1]^2
  epsil[i] = sig2[i]^.5 *zt[i]
  y[i] = 5+epsil[i] }
y = y[-(1:10)]
@
}
Pour obtenir les graphiques on lancera le code

{\small
\begin{verbatim}
plot.ts(y)
acf(y)
\end{verbatim}
}

\begin{Exercice} [Test d'hétéroscédasticité conditionnelle basé sur un test de blancheur]
Tester   l'hétéroscédasticité conditionnelle de ces deux séries par un test de blancheur de Box-Pierce.

\end{Exercice}

\textbf{Réponse.}
Les deux séries en question sont le processus ARCH(1) simulé et le bruit blanc à la base, colonnes 1 et  3 dans
\code{archsim.1} simulé section 12.3.1 suivant le modèle \ref{sim.arch1}. On en refait la simulation ici :


{\small
<<>>=
require(fGarch)
spec.1=garchSpec(model=list(mu=5,omega=0.1,alpha=0.9,beta=0),
   rseed=397)
archsim.1 =   garchSim(extended = TRUE,spec.1, n = 300, n.start=10)
head(archsim.1,2)
@
}


On a noté que l'hétéroscédasticité conditionnelle 
d'un bruit blanc $x_t$,  se traduit par la non blancheur de son carré,  un test de blancheur du carré de la série est
donc  un test d'homoscédasticité de la série. La blancheur de l'ARCH(1) simulé dans le livre et celle de la série $z_t$ sous-jacente
s'obtiennent par :

{\small
<<label=test.hetero.2>>=
moyenne=mean(archsim.1[,1])
ret = c(6,12,18)
a1=Box.test.2(archsim.1[,1],ret,type="Box-Pierce",decim=8)
a2=Box.test.2((archsim.1[,1]-moyenne)^2,ret,type="Box-Pierce",decim=8)
a3=Box.test.2(archsim.1[,3]^2,ret,type ="Box-Pierce",decim=8)
a123 = cbind(a1,a2[,2],a3[,2])
colnames(a123)=
   c("Retard","p.val y1","p.val y1 centré carré","p.val z_t")
@
}
On centré la série simulée car elle n'est pas de moyenne nulle.  
{\small
<<label=hetero.tex, results=tex, echo=FALSE>>=
require(xtable)
xtable(a123 , caption="Test de blancheur, p-values"  , label="blanc.y", digits=4)
@
}
\noindent
On a testé que les coefficients d'autocorrélation  jusqu'aux ordres 6, 12, 18 sont nuls, pour la série simulée \code{y1}, son carré centré,
et pour la série du bruit blanc gaussien servant à engendrer l'ARCH. On voit (table \ref{blanc.y}) qu'on accepte
l'hypothèse de blancheur pour la série simulée et  pour le bruit
blanc gaussien sous-jacent, mais qu'on la rejette évidemment pour le carré de la série centrée.


\begin{exercice}[Tests sur \code{mod1}]
L'estimation de (\ref{arch1.sim}) a été obtenue dans \code{mod1} :
\begin{eqnarray}  \label{arch1.sim}
    y_t &= & 2+ \sigma_t z_t \\
       \sigma^2_t& =& 0.09 + 0.15 \epsilon_{t-1}^2 +  0.3 \epsilon_{t-2}^2  + 0.4 \sigma^2_{t-1}. \notag
 \end{eqnarray}
 Faisons l'hypothèse que les estimateurs des paramètres sont approximativement 
normalement distribués. 
\begin{itemize}
\item  Extraire de la sortie de l'estimation,  la matrice des variances-covariances estimées des paramètres.
  \item Tester l'hypothèse que $\alpha_2 = 0.3$.
  \item Tester l'hypothèse que $(\alpha_2, \beta_1)  = (0.3, 0.4)$
 \item Conclure.
\end{itemize}

\end{exercice}

\textbf{Réponse.}
D'abord nous simulons la même série que dans le chapitre :


{\small
<<label=sim.garch>>=
spec=garchSpec(model=list(mu=2,omega=0.09,alpha =c(0.15, 0.3),
  beta = 0.4), rseed=9647)
var.margi = 0.09/(1 - 0.15 - 0.3-0.4)
y = garchSim(spec, n = 420, extended = TRUE)
y1 = y[21:420,1]
@
}
\noindent
La série à étudier est formée des 400 dernières   observations   contenues dans la première colonne de \verb"y".
Avant d'ajuster  un  modèle  à cette série nous avons examiné son chronogramme :

\begin{rcode}
plot.ts(y1, xlab='temps')
\end{rcode}


Il y a plusieurs valeurs extrêmes que  nous décidons de les remplacer par des valeurs plus raisonnable. 

{\small
<<label=y1.ext1, eval=TRUE>>=
m1 = mean(y1)
(q5 = quantile(abs(y1-m1), probs=c(.975,.98,.985,.99,.995)))
extrem985 = which(abs(y1-m1) > q5[3])
cat('nombre de points : ',length(extrem985),'\n')
y1b = y1
y1b[extrem985]  = q5[3]* sign(y1[extrem985]-m1)
@
}
\noindent
Nous travaillons sur la série corrigée \code{y1b}.

\begin{itemize}
\item  Extraction de la matrice des variances-covariances estimées des paramètres.
L'estimation est conduite par :

{\small
<<label=y1.ext2, eval=TRUE>>=
mod1=garchFit(~garch(2,1),data=y1b,trace=FALSE,include.mean=TRUE)
@
}
On situe les statistiques qui nous intéressent par \code{str(mod1)}. On y récupère aussi les noms précis des paramètres. 
L'estimation des paramètres alpha2 et beta1 est extraite de \code{mod1} par : 

{\small
<<>>=
param= c( "alpha2","beta1")
estim = as.matrix(coef(mod1)[param])
@
}
et celle de leur matrice des covariances estimée par :

{\small
<<>>=
# matrice des covariances des estimateurs
vcov.par = mod1@fit$cvar[param,param]
@
}
On peut noter également que \verb"mod1@fit$se.coef" contient les écarts-types d'estimation, {\ie}~la racine carrée de la diagonale de
la matrice des covariances.

 \item Tester l'hypothèse H$_0$ : $\alpha_2 = 0.3$.

 Nous devons former la statistique :
$T = \frac{\widehat{\alpha}_2 - 0.3}{s_{\widehat{\alpha}_2}}$ (cf. chap.3). 
Sous H$_0$, elle  est approximativement normalement distribuée $\nor(0,1)$.
 
 {\small
<<label=test.par.2>>=
t.alpha =  (coef(mod1)["alpha2"]-0.3)/mod1@fit$se.coef["alpha2"]
p.val = 2* pnorm(-abs(t.alpha))
cat("p-value : ",p.val,"\n")
@
}
\noindent
 La p-value est   élevée,
et donc l'estimation est cohérente avec le modèle simulé.


\item Tester l'hypothèse que $(\alpha_2, \beta_1)  = (0.3, 0.4)$.

Sous l'hypothèse nulle :
\[
\mbox{H}_0 : \begin{bmatrix}
\alpha_2 \\
\beta_1
\end{bmatrix} = \begin{bmatrix}
0.3 \\
0.4
\end{bmatrix},\quad \begin{bmatrix}
\widehat{\alpha}_2 \\
\widehat{\beta_1}
\end{bmatrix}  \sim \mbox{~AN}(\begin{bmatrix}
0.3 \\
0.4
\end{bmatrix},\widehat{V}),
\]
où  $\widehat{V}$ est la matrice des covariances estimée pour ce couple d'estimateurs et
\[
[\widehat{\alpha}_2 - 0.3 , \widehat{\beta_1} - 0.4] \widehat{V}^{-1} \begin{bmatrix}
\widehat{\alpha}_2 -0.3 \\
\widehat{\beta_1} -0.4
\end{bmatrix}  \sim \mbox{~A}\chi^2_2
\]
On rejette l'hypothèse nulle pour de grandes valeurs de cette statistique. Pour effectuer ce test,
il nous faut   : (1) récupérer les estimations de
ces paramètres ainsi que  la matrice des covariances estimée correspondante, ce que nous avons fait au début de l'exercice, (2)
calculer la distance du $\chi^2$ de la moyenne estimée à la moyenne théorique, et enfin (3) la p-value.

{\small
<<label=test.par.1>>=
# moyenne théorique
theo = as.matrix(c(0.3, 0.4))
# difference
differ = estim-theo
# stat de test
khi = t(differ)%*%solve(vcov.par)%*%differ;
# statistique de test et p-value
cat("stat de test et p-value : ",c(khi, 1-pchisq(khi,df=2)),"\n")
@
}
\noindent

  item En résumé, la simplification d'estimation envisagée pour ces deux paramètres est cohérente avec le modèle simulé.
Il faut cependant  garder à l'esprit que ces estimations de covariances sont basées sur des approximations grossières et donc les 
conclusions des tests sont d'abord à comprendre comme des indications.

\end{itemize}


\begin{Exercice}[L'Oréal]

Dans le traitement de L'Oréal, on a enchaîné la modélisation ARMA de la série et la modélisation GARCH du résidu de cet ajustement.
\begin{itemize}
  \item Effectuer une modélisation GARCH du rendement sans passer par l'étape de modélisation par un ARMA.
  \item Superposer sur un même graphique : la série à prédire et les intervalles de prévision (à 90\%) par un ARMA  et  par un GARCH. Commenter.
\end{itemize}

\end{Exercice}

\textbf{Réponse.}

\begin{itemize}
  \item Effectuer une modélisation GARCH du rendement sans passer par l'étape de modélisation par un ARMA.
  
  
Dans le chapitre, nous avons obtenu le modèle 12.13 en enchaînant une modélisation ARMA du rendement puis une modélisation GARCH 
du résidu du premier ajustement. Tout d'abord, nous réestimons le modèle ARMA identifié pour L'Oréal :

{\small
<<label=lor.1a, echo=TRUE>>=
require(fBasics)
data(csdl)
aa = returns(csdl, percentage = TRUE)
aab = aa[complete.cases(aa) == TRUE,]
r.csdl = its(aab, as.POSIXct(row.names(aab)))
r.lor <- rangeIts(r.csdl[,"L_Oreal"], start= "2007-12-28")
r.lor.0 = r.lor[1:(length(r.lor)-51)]
r.lor.1 = r.lor[(length(r.lor)-50):length(r.lor)]
mod.r.lor=Arima(r.lor.0@.Data,order=c(0,0,4),include.mean=FALSE,
                fixed=c(NA,NA,0,NA))
@
}

D'autre part, nous estimons sur le rendement, le modèle GARCH identifié sur les résidus 
de l'ajustement à un ARMA :

{\small
<<label=lor.12, echo=TRUE>>=
<<label=esti.var>>=
var.marg.est<-function(mod)
{param.estim = mod@fit$par
 std.estim = mod@fit$se.coef
 k<-which(names(param.estim)=="omega")
 value = param.estim[k]/(1-sum(param.estim[(k+1):length(param.estim)]))
cat("variance marginale : ",value,"\n")
}
mod.garch.lor=garchFit(~garch(1,1),data=r.lor.0@.Data,trace=FALSE,
   include.mean=TRUE,na.action=na.pass)
summary(mod.garch.lor)
var.marg.est(mod.garch.lor)
@
}
Nous pouvons voir que le modèle capte bien l'hétéroscédasticité du rendement et que les paramètres sont significatifs.
Mais le modèle fournit un résidu qui  n'est pas blanc, au contraire de ce qu'avait fourni l'ajustement d'un ARMA/GARCH
section 12.8.1.



  \item Superposer sur un même graphique : la série à prédire et les intervalles de prévision (à 90\%) par un ARMA  et  par un GARCH. Commenter.

Il faut donc prédire le rendement de L'Oréal par les deux méthodes.


{\small
<<label=stock.lor>>=
npred=50
pred.arima.lor=predict(mod.r.lor,n.ahead=npred)
pred.garch.lor=predict(mod.garch.lor,n.ahead=npred)

str(pred.arima.lor)
demi = qnorm(0.95)*pred.arima.lor$se
binf.arima.lor =  pred.arima.lor$pred-demi
bsup.arima.lor =  pred.arima.lor$pred+demi

demi = qnorm(0.95)*pred.garch.lor$standardDeviation
binf.garch.lor =  pred.garch.lor$meanForecast-demi
bsup.garch.lor =  pred.garch.lor$meanForecast+demi
@
}
 \code{mat.lor} rassemble  les différentes séries dont on veut superposer les chronogrammes :
l'estimation du rendement moyen, le rendement observé, les bornes inférieures et supérieures obtenues par la modélisation GARCH
puis par la modélisation ARMA.

{\small
<<label=ajust,height=10,fig=TRUE,echo=TRUE,eval=FALSE>>=
par(oma=rep(0.5,4))
mat.lor = cbind(binf.arima.lor,bsup.arima.lor, 
 binf.garch.lor,bsup.garch.lor,r.lor.1[1:npred])
matplot(1:npred,mat.lor,type='l', col='black', lty=c(1,1,2,2,3),
  lwd=2,xlab="horizon 50", ylab="rendement")
leg.txt = c("GARCH","AR","réalisation")
legend(14,3, leg.txt, lty=c(1,2,3))
@
}
\noindent

On note sur le graphique superposant les intervalles de prévision par les deux méthodes (fig. \ref{previreal.Loreal}) que la seule modélisation
par un ARMA fournit des intervalles de prédiction trop amples.


\noindent
\begin{figure}
\begin{center}
<<label=previreal.Loreal, echo=FALSE,results=tex>>=
<<bfig5>>
<<ajust>>
<<zfig2>>
<<bfigps5>>
<<ajust>>
<<zfig2>>
<<zfiginclude>>
@
\end{center}
\caption{L'Oréal : intervalles de prévision (ARMA et GARCH) à 80\% et réalisation du rendement.}\label{previreal.Loreal}
\end{figure}
%##############################  Fin L'Oréal #############################################

\end{itemize}


\begin{Exercice}[Danone]

Dans le traitement de Danone, on a enchaîné la modélisation ARMA de la série et la modélisation GARCH du résidu de cet ajustement.
\begin{itemize}
  \item Effectuer une modélisation GARCH du rendement sans passer par l'étape de modélisation par un ARMA.
  \item Superposer sur un même graphique : la série à prédire et les intervalles de prévision (à 90\%) par un ARMA, un ARMA-GARCH et   un GARCH. Commenter.
\end{itemize}
\end{Exercice}

\textbf{Réponse.}

\begin{itemize}
  \item Effectuer une modélisation GARCH du rendement sans passer par l'étape de modélisation par un ARMA.
Dans le chapitre, nous avons obtenu le modèle 12.16 en enchaînant une modélisation ARMA du rendement puis une modélisation GARCH 
du résidu du premier ajustement.

{\small
<<label=rdan.0>>=
r.dan = rangeIts(r.csdl[,"Danone"], start= "2007-12-28")
r.dan.0 = r.dan[1:(length(r.dan)-50)]
r.dan.1 = r.dan[(length(r.dan)-49):length(r.dan)]
@
}

{\small
<<label=armagarch.dan,echo=FALSE>>=
mod.arma.dan=Arima(r.dan.0@.Data, order=c(2,0,0),include.mean= FALSE)
mod.garch.dan=garchFit(~garch(1,1),data=r.dan.0@.Data,trace=FALSE,
               include.mean=FALSE,na.action=na.pass)
@
}
 
  \item Superposer sur un même graphique : la série à prédire et les intervalles de prévision (à 90\%) par un ARMA  et  par un GARCH. Commenter.


Nous calculons les prédictions par les deux modèles.


{\small
<<label=stock.dan>>=
npred=50
pred.arima.dan=predict(mod.arma.dan,n.ahead=npred)
pred.garch.dan=predict(mod.garch.dan,n.ahead=npred)
str(pred.arima.dan)
demi = qnorm(0.95)*pred.arima.dan$se
binf.arima.dan =  pred.arima.dan$pred-demi
bsup.arima.dan =  pred.arima.dan$pred+demi
#
demi = qnorm(0.95)*pred.garch.dan$standardDeviation
binf.garch.dan =  pred.garch.dan$meanForecast-demi
bsup.garch.dan =  pred.garch.dan$meanForecast+demi
@
}
 \code{mat.dan} rassemble  les différentes séries dont on veut superposer les chronogrammes :
le rendement observé et les bornes inférieures et supérieures obtenues par la modélisation GARCH
puis par la modélisation ARMA.

{\small
<<label=pred.dan,height=10,fig=TRUE,echo=TRUE,eval=FALSE>>=
par(oma=rep(0.5,4))
mat.dan = cbind(binf.arima.dan,bsup.arima.dan, 
 binf.garch.dan,bsup.garch.dan,r.dan.1[1:npred])
matplot(1:npred,mat.dan,type='l', col='black', lty=c(1,1,2,2,3),
  lwd=2,xlab="horizon 50", ylab="rendement")
leg.txt = c("GARCH","AR","réalisation")
legend(14,3, leg.txt, lty=c(1,2,3))
@
}
\noindent
Nous observons que pour Danone  il y a peu de différence dans les prévisions GARCH et ARMA (fig. \ref{pred.dan}).

\begin{figure}
\begin{center}
<<label=pred.dan.plot,  echo=FALSE,results=tex>>=
<<bfig5>>
<<pred.dan>>
<<zfig2>>
<<bfigps5>>
<<pred.dan>>
<<zfig2>>
<<zfiginclude>>
@
\caption{Danone : prévision ARMA et GARCH (90\%) et réalisation.}\label{pred.dan}
\end{center}
\end{figure}
\end{itemize}



%% %%%%%%%%%%%%%%%%%%%%%%%%%%% section essilor
%\section{Effets d'une valeur aberrante \label{sec.aber}}
%Nous avons vu au chapitre 2 que le cours d'Essilor présente un point aberrant et en avons envisagé différentes corrections.
%Ici, nous le corrigeons en le rempla\c{c}ant par la moyenne de ses voisins.
%
%{\small
%<<label=essil.aber>>=
%data(essil)
%i.max = which(essil == max(essil))
%essil[i.max]
%essil1 = essil
%essil1@.Data[i.max] = 0.5*(essil@.Data[i.max-1]+essil@.Data[i.max+1])
%@
%}
%\noindent
%Examinons maintenant l'effet de ce point sur différentes statistiques en les calculant pour la série originale et la série corrigée.
%Nous faisons finir la  première période au 30 juin 2007.
%
%{\small
%<<label=essil.aber2>>=
%resume = function(cours, titre="")
%{
% avan.06 = rangeIts(cours, end= "2007-06-30")
% apres.06 = rangeIts(cours, start= "2007-07-01")
% rendav.06 = returns(avan.06,percentage= TRUE)
% rendapr.06 = returns(apres.06,percentage= TRUE)
% colnames(rendapr.06) = colnames(cours)
% sk.av06   =  apply(rendav.06,2,skewness, na.rm=TRUE)
% kurt.av06 =  apply(rendav.06,2,kurtosis, na.rm=TRUE)
% sk.apr06  =  apply(rendapr.06,2,skewness, na.rm=TRUE)
% kurt.apr06=  apply(rendapr.06,2,kurtosis, na.rm=TRUE)
% sk06 =  rbind(sk.av06,  sk.apr06, kurt.av06 , kurt.apr06)
% colnames(sk06) =  titre
% rownames(sk06) = c("asym.av","asym.apr","aplat.av","aplat.apr")
% sk06
%}
%non=resume(essil, "série non corrigée")
%oui=resume(essil1, "série corrigée")
%@
%}
%
%<<label=essil.aber3,results=tex, echo=FALSE>>=
%res.comp= matrix(NA, ncol=3, nrow=2)
%res.comp[,1]= c(non[c(1,3)])
%res.comp[,2]= c(oui[c(1,3)])
%res.comp[,3]=c(oui[c(2,4)])
%rownames(res.comp)=c("Asym.","Aplat.")
%colnames(res.comp)=c("Non cor. AV","Cor. AV","Après")
%xtable(res.comp, caption="Asymétrie et aplatissement, avant (sans et avec correction)et après"  , label="avapr", digits=4)
%@
%
%Nous  constatons,  table (\ref{avapr}), que la valeur aberrante modifie très sensiblement le coefficient d'aplatissement.
%
\end{document}